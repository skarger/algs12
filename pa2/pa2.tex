\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\usepackage{enumerate}

\title{CS E124 Programming Assignment 2}
\author{Stephen Karger}
\date{March 31, 2012}                                           % Activate to display a given date or no date

\begin{document}
\maketitle

\section{Analysis of optimal cross-over point}
Assuming that the cost of any single arithmetic operation (adding, subtracting, multiplying, or dividing two real numbers) is 1, the following equations model conventional matrix multiplication and Strassen's algorithm.
\begin{flalign*}
\text{Conventional: } & 2n^{3} - n^{2} & \\
\text{Strassen: } & 6n^{2 + log(7/4)} + n^{log\,7} - 6n^{2} &
\end{flalign*}
Derivation: \\
For each individual entry in the matrix product, the conventional algorithm performs $n$ multiplications. Then it takes $n-1$ additions to combine them, for $2n - 1$ total operations. Since there are $n^2$ elements to compute, the number of operations for the entire algorithm is $2n^3 - n^2$. \\
\\
In Strassen's algorithm multiplying an $n$ x $n$ matrix involves 10 additions or subtractions of dimension $n/2$ matrices to compute $P_1$ through $P_7$, then 7 multiplications of dimension $n/2$ matrices using $P_1 \dots P_7$, and finally 8 additions of dimension $n/2$ matrices to combine the results. Adding matrices of dimension $n/2$ requires $(\frac{n}{2})^2$ real number additions, so a recurrence relation for the number of multiplications is $T(n) = 18(\frac{n}{2})^2 + 7T(n/2)$. \\
\\
Clearly $T(1) = 1$. Expanding the recurrence relations yields
\begin{flalign*}
T(n) &= 18 \left (\frac{n}{2} \right )^2 + 7T(n/2) = 18 \left (\frac{n}{2} \right )^2 + 7\ \left [  18 \left (\frac{n}{4} \right )^2 + 7T(n/4)  \right ] & \\
&= 18 \left (\frac{n}{2} \right )^2 + 7\ \left [  18 \left (\frac{n}{4} \right )^2 + 7 \left [ 18 \left (\frac{n}{8} \right )^{2} + 7T(n/8)  \right ]  \right ] & \\
&= 18 \left (\frac{n}{2} \right )^2 + 7*18 \left (\frac{n}{4} \right )^2 + 7^2*18 \left (\frac{n}{8} \right )^{2} + \cdots + 7^{log\,n - 1} \left (18 \left ( \frac{n}{2^{log\,n}} \right )^{2} + 7  \left ( \frac{n}{2^{log\,n}} \right )^{2} \right ) & \\
&= 7^{log\,n} + \sum_{0}^{log\,n - 1} 7^i * 18 \left (\frac{n}{2^{i+1}} \right )^2 = 7^{log\,n} + 18 \left (\frac{n}{2} \right )^2 \sum_{0}^{log\,n - 1} \left ( \frac{7}{4} \right )^{i}
\end{flalign*}
\\
Using the geometric series formula for the sum above gives
\begin{flalign*}
T(n) &= 7^{log\,n} + 18 \left (\frac{n}{2} \right )^2 \sum_{0}^{log\,n - 1} \left ( \frac{7}{4} \right )^{i} = 7^{log\,n} + 18 \left ( \frac{n}{2} \right )^2 * \frac{1 - \left ( \frac{7}{4} \right )^{log\,n} } {1 - \frac{7}{4} }
\end{flalign*}
Finally, using the fact that $a^{log\,b} = b^{log\,a}$ and simplifying algebraically 
\begin{flalign*}
T(n) &= n^{log\,7} + 18 \left ( \frac{n}{2} \right )^2 * \frac{1 - n^{log \left ( \frac{7}{4} \right )} } { 1 - \left ( \frac{7}{4} \right ) } =
n^{log\,7} + 18 \left ( \frac{n}{2} \right )^2 * \frac{n^{log \left ( \frac{7}{4} \right )}} { \frac{7}{4} - 1 } \\
&= n^{log\,7} + 18 \left ( \frac{n^{2}}{4} \right ) * \frac{ n^{log \left ( \frac{7}{4} \right )} - 1} { \frac{3}{4} } = 
n^{log\,7} + 6n^{2} \left ( n^{log \left ( \frac{7}{4} \right )} - 1 \right ) \\
&= n^{log\,7} + 6n^{2 + log \left ( \frac{7}{4} \right )} - 6n^2 \\
&= 6n^{2 + log(7/4)} + n^{log\,7} - 6n^{2}
\end{flalign*}
Comparing these two functions, Strassen's algorithm requires more operations than the conventional method until the matrices have dimension 655, so the estimate for $n_0 = 655$. \\

\begin{tabular}{l|l|l}
n & Conventional & Strassen \\  \hline
650 & 548827500 & 549471506 \\
651 & 551365101 & 551851137 \\
652 & 553910512 & 554237392 \\
653 & 556463745 & 556630282 \\
654 & 559024812 & 559029813 \\
655 & 561593725 & 561435994 \\
656 & 564170496 & 563848833 \\
657 & 566755137 & 566268339 \\
658 & 569347660 & 568694520 \\
659 & 571948077 & 571127384 \\
660 & 574556400 & 573566938
\end{tabular}
\\
\section{Implementation and experimental results}
\begin{tabular}{l|l|l|l}
n & $n_0$ & Conventional (m sec) & Strassen (m sec) \\  \hline
256 & 255 & 26 & 66 \\
512 & 511 & 198 & 228 \\
1024 & 655 & 1573 & 1477 \\
2048 & 655 & 13317 & 10086 \\
2048 & 1025 & 13428 & 11238 \\
4096 & 655 & 105000 & 70054 \\
4096 & 1025 & 105305 & 78530 \\
4096 & 2049 & 104974 & 94452
\end{tabular}
\\
\\
\\
Experimentally I found that 655 is still the optimal cutoff value. (Actually it may be somewhere between 512 - 1023, but my implementation only supports dimensions that are powers of 2, so I cannot investigate at a finer resolution.) For matrices of dimension less than 655 (512) the conventional algorithm is faster. As I increase the cutoff, Strassen's algorithm remains faster than the conventional algorithm, but not quite as fast as when using $n_0 = 655$. \\
\\
Optimizations:  \\
For Strassen's algorithm I followed the hints to avoid unnecessary memory allocation and to avoid copying large amounts of data. \\
\\
A more straightforward implementation would allocate memory for each intermediate calculation at the moment it is needed, not before.  In contrast my implementation allocates all memory used for the intermediate calculations ahead of time, based on the dimension of the input matrices. Furthermore, for a given dimension $k < n$, Strassen's algorithm will multiply some number of sub-matrices with dimension $k$. I do not allocate distinct memory for all of those multiplications. Instead, I allocate enough memory for the sub-calculations involved in \emph{one} multiplication of matrices with dimension $k$. Then I re-use that memory for computing each sub-matrix, one-by-one.
\\
\\
To avoid copying, I created a bookkeeping object, MxMap, that provides the coordinates of the data to multiply within a given matrix, or the coordinates on the destination matrix where the results should go. Ultimately the four sub-sums of any matrix product are computed on the product matrix itself, up to and including the final matrix returned to the caller. Results are never copied from one piece of storage to another.
\\
\\
For conventional multiplication I used a looping order that improved over the naive order by a factor of 6. Instead of computing each element fully before moving on to the next, I partially compute a sum for every element in each row of the product. Evidently this allows more data to be accessed directly from the CPU cache than the naive method. \\
\begin{verbatim}
        for (int i = 0; i < n; i++)
            for (int j = 0; j < n; j++)
                for (int k = 0; k < n; k++) {
                    // conventional path:
                    C.data[i][j] +=  (A.data[i][k] * B.data[k][j]);
                    // superior cache performance path:
                    C.data[i][k] +=  (A.data[i][j] * B.data[j][k]);
                }               
\end{verbatim}


\end{document} 
\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{algorithmic}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}



\title{}
\author{}
\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

\section{Dynamic programming algorithm}
To minimize the residue we need to divide the numbers into two sets such that their sums are as close to equal as possible. This is equivalent to finding a subset that adds up to half the total sum, or as close to it as possible. In other words we need to solve the Subset Sum problem. \\
\\
More precisely, given $A = (a_1,a_2,\dots,a_n)$, compute $b = \sum_1^n a_i$, set $W = b/2$, and determine the subset $S \subset A$ that maximizes $\sum_{a \in S} a_i$, under the constraint that $\sum_{a \in S} a_i \le W$.\\
\\
Note: I consulted Kleinberg and Tardos, pages 267-271, and am using their solution to the Subset Sum problem: \\
\begin{algorithmic}
\STATE Function Subset-Sum($A, n, W$)
\STATE Array $M[0, \dots, n, 0, \dots, W]$
\STATE Initialize $M[0, w] = 0$ for each $w = 0, 1, \dots, W$
\FOR{$i=1, 2, \dots, n$}
\FOR{$w = 0, \dots, W$}
\STATE OPT represents that largest sum possible with the constraint that $M[i, w] \le w$
\IF{$w < a_i$}
\STATE $M[i, w]$ := OPT($i-1,w$)
\ELSE
\STATE $M[i, w]$ := max(OPT($i-1,w$), $a_i + $ OPT($i-1,w-a_i$))
\ENDIF
\ENDFOR
\ENDFOR
\STATE Return array $M$
\STATE End Subset-Sum \\
\end{algorithmic}
\leavevmode
\\
The algorithm above fills in a 2d array with the maximum possible sums for every non-negative integer value $w \le W$. For a given set item $a_i$ it checks whether $a_i > w$. If so then $a_i$ cannot be part of the optimal sum for that $w$. Otherwise it determines the optimal sum composed of $a_i$ and previous set items. At this point it has already computed the optimal sums that use previous set items and smaller constraints. In particular it has computed the optimal sum that use previous set items and the constraint $w' = w - a_i$.\\
\\
Given the filled array $M$ we can determine the subset $S$ as follows. The starting input to the algorithm below would be $M$, $j=n$, and $w = W$. It checks if $a_j$ augments the optimal sum possible for $W$. If not it tries the same check for the previous element $a_{j-1}$. If so then it includes $j$ in the input, and checks if element $a_{j-1}$ augments the optimal sum possible for $W - a_j$. Continuing in this fashion it grabs every $j$ for which we should include $a_j$ in $S$.\\
\begin{algorithmic}
\STATE Function Find-Subset($M, j, w$)
\IF{j = 0}
\STATE Return empty set
\ELSIF{$M[j,w] \ge M[j-1,w]$}
\STATE Return $j\ \cup $ Find-Subset($M,j-1,w-a_j$)
\ELSE
\STATE  Find-Subset($M,j-1,w$)
\ENDIF
\STATE End Find-Subset
\end{algorithmic}
\leavevmode
\\
The algorithm for the Number Partition problem is then just
\begin{algorithmic}
\STATE Function Number-Partition($A, n$)
\STATE $b := \sum_1^n a_i$
\STATE $W := b/2$
\STATE Array $M := $ Subset-Sum($A, n, W$)
\STATE $S'$ := Find-Subset($M,n,W$)
\STATE $S := []$ 
\FORALL{$j$}
\IF{$j \in S'$}
\STATE $S_j := 1$
\ELSE
\STATE $S_j := -1$
\ENDIF
\ENDFOR
\STATE Return $S$
\STATE End Number-Partition
\end{algorithmic}
\leavevmode
\\
As far as running time: We can compute $b$ and $W$ in $O(n\,log \,b)$ time. After computing $M$ we can find the optimal subset in $O(n)$ time. The dominating task is Subset-Sum. It loops over the $n$ items in $A$, and using $W+1$ constraint values does constant work. $W$ is $O(b)$ and thus Subset-Sum's running time is $O(nb)$. 
\\
\\
\section{How to implement $O(n\,log\,n)$ KK algorithm}
The KK-algorithm repeatedly needs to find the largest and second largest remaining elements. A good data structure to accomplish that is a Max-Heap. Given $A$, the unsorted list of $n$ integers, we can run Build-Heap in $O(n\,log\,n)$ time (or even $O(n)$ time, using the tight bound). \\
\\
For a single differencing operation we call Extract-Max twice to get the two largest elements. We find their difference, and it is assumed that the numbers are small enough to do that in constant time. Then to complete the operation we call Insert once to put the result back in the heap. Extract-Max and Insert run in $O(log\,n)$ time so the entire differencing operation is $O(log\,n)$. Note that if the difference of two elements is 0 then there is no need to insert 0 into the heap, so the running time might be slightly better, although still asymptotically equivalent.\\
\\
The KK-algorithm must perform differencing operations until the heap has only one item whose value is the residue. Each differencing operation reduces the size of the heap by at least one, so we need to difference $O(n)$ times. This means the algorithm's total running time is $O(n\,log\,n)$.
\\
\\
\newpage
\section{Experimental Results}
The table below shows the residues computed using the various algorithms (time data on later pages). \\
The pre-partitioning method is the clear winner across the board, and it does best with the repeated random heuristic. With the pre-partition method hill climbing appears to do slightly better than simulated annealing. \\
\\
After pre-partitioning, the best residues are computed using the plain KK algorithm and the repeated random heuristic using the standard solution representation. Those two obtain residues of roughly the same orders of magnitude, although they don't necessarily obtain the same order of magnitude on a given problem. \\
\\
Hill climbing and simulated annealing do not obtain good results using the standard solution representation. This suggests that either this problem has multiple local optima or that moving from neighbor to neighbor moves incredibly slowly (i.e., 25,000 iterations is not enough to make good progress). \\
It's surprising however that hill climbing and simulated annealing \emph{do} work quite well when using pre-partitioning. I believe that is due to the fact that pre-partitioning creates larger jumps from one neighbor to another than when moving between standard solution neighbors.
\\
\\
\begin{tabular}{l r l r l r l}
 & \multicolumn{2}{c}{Repeated Random} & \multicolumn{2}{c}{Hill Climbing} & \multicolumn{2}{c}{Simulated Annealing} \\ \hline
KK & Standard & PP & Standard & PP & Standard & PP \\ \hline
33603 & 143869 & 103 & 617722979 & 101 & 163997777 & 323 \\
254125 & 67177 & 133 & 221852557 & 97 & 1026726627 & 1291 \\
152538 & 48758 & 464 & 471108360 & 254 & 383806292 & 764 \\
4674 & 84432 & 84 & 126639230 & 756 & 79579640 & 14 \\
350136 & 61928 & 172 & 501639462 & 792 & 513721612 & 1616 \\
58844 & 37160 & 308 & 120490554 & 418 & 286524904 & 428 \\
17554 & 3868 & 18 & 869665142 & 530 & 381436272 & 1574 \\
370179 & 95989 & 235 & 767086505 & 1645 & 236879231 & 923 \\
183049 & 120463 & 199 & 59672109 & 965 & 107650177 & 265 \\
184765 & 13167 & 77 & 122902431 & 1001 & 118757165 & 1625 \\
120649 & 8313 & 39 & 107809115 & 11 & 586137251 & 603 \\
963452 & 99454 & 2 & 312457212 & 558 & 434972022 & 3484 \\
116191 & 4827 & 123 & 35573269 & 355 & 5745243 & 2169 \\
136370 & 26660 & 120 & 101713178 & 102 & 748537410 & 360 \\
434604 & 49364 & 88 & 172863674 & 312 & 57934834 & 524 \\
412106 & 147550 & 274 & 143222652 & 724 & 145245682 & 1612 \\

\end{tabular}
\\
\\
Continued next page\\
\\ Computed residues, continued \\
\\
\begin{tabular}{l r l r l r l}
 & \multicolumn{2}{c}{Repeated Random} & \multicolumn{2}{c}{Hill Climbing} & \multicolumn{2}{c}{Simulated Annealing} \\ \hline
KK & Standard & PP & Standard & PP & Standard & PP \\ \hline
68196 & 57218 & 86 & 549324184 & 534 & 127387322 & 1016 \\
49061 & 118477 & 287 & 64126153 & 651 & 362687259 & 1597 \\
43145 & 134353 & 79 & 107333769 & 21 & 176837641 & 1235 \\
5925 & 95641 & 55 & 557503503 & 1041 & 296212947 & 3613 \\
80856 & 11816 & 232 & 619711992 & 152 & 105467618 & 162 \\
307417 & 142961 & 431 & 78651517 & 559 & 350751591 & 325 \\
160303 & 62325 & 21 & 444943125 & 625 & 99329781 & 1041 \\
144216 & 97468 & 98 & 471798796 & 1082 & 128891504 & 1722 \\
16312 & 24722 & 128 & 556232004 & 644 & 329379304 & 1702 \\
18729 & 139863 & 607 & 480777655 & 443 & 9138891 & 99 \\
218151 & 71321 & 57 & 590730889 & 1051 & 313304505 & 235 \\
105493 & 23061 & 167 & 224939745 & 1361 & 62529337 & 907 \\
157258 & 76028 & 204 & 262751392 & 106 & 53642224 & 316 \\
333193 & 7721 & 119 & 25229529 & 5041 & 209267149 & 251 \\
34799 & 98635 & 7 & 76566599 & 267 & 190288203 & 1491 \\
674262 & 181876 & 62 & 12343242 & 70 & 672000836 & 1092 \\
756178 & 49352 & 140 & 17325692 & 30 & 49054186 & 4358 \\
204693 & 11665 & 161 & 307321817 & 933 & 19274457 & 609 \\
28144 & 5314 & 300 & 628192368 & 188 & 501322808 & 496 \\
128090 & 12618 & 192 & 169321560 & 1340 & 6655202 & 120 \\
282493 & 121491 & 129 & 209258783 & 239 & 289685281 & 2087 \\
42841 & 10439 & 153 & 41531505 & 125 & 58023587 & 561 \\
137562 & 40866 & 8 & 489275874 & 62 & 21394630 & 128 \\
532958 & 40770 & 144 & 506251476 & 254 & 7177694 & 1196 \\
81631 & 45407 & 277 & 9593577 & 69 & 386955999 & 1945 \\
152398 & 12490 & 126 & 49103934 & 2042 & 253762736 & 1150 \\
297472 & 34768 & 140 & 58074990 & 14 & 91188336 & 278 \\
6203096 & 12540 & 326 & 925092766 & 338 & 170830442 & 146 \\
139145 & 104991 & 269 & 7377059 & 3225 & 102300089 & 1095 \\
8178 & 12688 & 136 & 122267794 & 2214 & 957279562 & 88 \\
31835 & 59265 & 659 & 41520603 & 787 & 353486759 & 15 \\
459813 & 21231 & 113 & 468137669 & 301 & 520676493 & 1057 \\
47043 & 172921 & 181 & 214804189 & 291 & 558526621 & 2241 \\
148415 & 120243 & 91 & 88000319 & 297 & 35199303 & 217 \\
\end{tabular}
\\
\\
\\
Unsurprisingly, the pre-partitioning method requires more time because every time the solution changes, whether moving to an entirely new random solution or simply moving to a neighbor, we have to re-run the KK algorithm to obtain the residue. By contrast when moving to a neighbor using the standard solution representation we can update the residue in constant time.\\
\\
Moving to another random solution using the standard representation takes a little longer than a neighbor, but still less than pre-partitioning. Computing the residue from the standard solution representation requires $O(n\,log\,b)$ time, where $b$ is the sum of numbers in the set. Computing the residue with the KK algorithm is $O(n\, log(b)\,log(n))$. \\
\\
A result of note is that hill climbing requires the least time, except for the plain KK algorithm itself. This makes sense because it re-computes the residue the least amount of times - only when a neighbor offers an improvement. Because simulated annealing sometimes moves to an inferior neighbor it involves more residue computations. In fact, when using pre-partitioning simulated annealing takes (slightly) longer than repeated random solutions. Possibly the increased overhead of computing the cooling probability contributes to that result. \\
\\
Computed Time in milliseconds \\
\\
\begin{tabular}{l r l r l r l}
 & \multicolumn{2}{c}{Repeated Random} & \multicolumn{2}{c}{Hill Climbing} & \multicolumn{2}{c}{Simulated Annealing} \\ \hline
KK & Standard & PP & Standard & PP & Standard & PP \\ \hline
7 & 129 & 960 & 16 & 775 & 26 & 995 \\ \hline
2 & 144 & 1039 & 16 & 851 & 30 & 1165 \\ 
1 & 159 & 982 & 17 & 815 & 25 & 1024 \\ 
6 & 132 & 989 & 17 & 821 & 29 & 1020 \\ 
2 & 137 & 1003 & 16 & 787 & 25 & 1035 \\ 
2 & 137 & 1066 & 18 & 872 & 25 & 1142 \\ 
2 & 143 & 984 & 17 & 798 & 25 & 1023 \\ 
1 & 130 & 990 & 17 & 801 & 25 & 1013 \\ 
2 & 140 & 989 & 16 & 777 & 25 & 1012 \\ 
1 & 152 & 974 & 17 & 815 & 32 & 982 \\ 
1 & 141 & 963 & 16 & 777 & 25 & 1009 \\ 
2 & 152 & 970 & 17 & 774 & 25 & 1013 \\ 
2 & 129 & 980 & 17 & 815 & 27 & 1013 \\ 
2 & 131 & 901 & 24 & 757 & 29 & 947 \\ 
2 & 137 & 889 & 16 & 723 & 27 & 912 \\ 
\end{tabular}
\\
\\
\\
Continued next page
\newpage
\begin{tabular}{l r l r l r l}
 & \multicolumn{2}{c}{Repeated Random} & \multicolumn{2}{c}{Hill Climbing} & \multicolumn{2}{c}{Simulated Annealing} \\ \hline
KK & Standard & PP & Standard & PP & Standard & PP \\ \hline
2 & 141 & 1005 & 24 & 801 & 30 & 1058 \\ 
2 & 137 & 987 & 29 & 773 & 29 & 1051 \\ 
2 & 138 & 1017 & 18 & 845 & 30 & 1049 \\ 
1 & 135 & 904 & 26 & 717 & 30 & 909 \\ 
1 & 136 & 910 & 23 & 747 & 35 & 924 \\ 
1 & 135 & 995 & 24 & 777 & 28 & 985 \\ 
2 & 147 & 984 & 16 & 788 & 25 & 1069 \\ 
2 & 138 & 996 & 18 & 830 & 31 & 1010 \\ 
1 & 140 & 996 & 16 & 811 & 29 & 1017 \\ 
1 & 137 & 1085 & 29 & 758 & 43 & 1019 \\ 
5 & 140 & 897 & 16 & 705 & 27 & 915 \\ 
1 & 139 & 1045 & 26 & 820 & 29 & 1030 \\ 
2 & 159 & 885 & 25 & 732 & 31 & 916 \\ 
1 & 131 & 1004 & 17 & 774 & 25 & 1036 \\ 
1 & 148 & 1011 & 17 & 822 & 25 & 1038 \\ 
1 & 135 & 985 & 17 & 806 & 26 & 1051 \\ 
2 & 135 & 993 & 17 & 841 & 27 & 1000 \\ 
1 & 130 & 1055 & 18 & 855 & 28 & 1101 \\ 
3 & 133 & 986 & 17 & 821 & 25 & 1035 \\ 
2 & 136 & 1002 & 16 & 830 & 25 & 1055 \\ 
2 & 133 & 902 & 24 & 719 & 29 & 898 \\ 
2 & 135 & 899 & 17 & 777 & 30 & 893 \\ 
1 & 134 & 877 & 28 & 729 & 34 & 910 \\ 
5 & 132 & 980 & 16 & 792 & 24 & 1034 \\ 
1 & 145 & 1081 & 18 & 848 & 29 & 1123 \\ 
2 & 132 & 1002 & 17 & 822 & 24 & 1029 \\ 
6 & 136 & 1072 & 18 & 915 & 28 & 1126 \\ 
2 & 158 & 972 & 25 & 786 & 30 & 1000 \\ 
6 & 132 & 973 & 16 & 820 & 25 & 1017 \\ 
2 & 135 & 902 & 17 & 718 & 28 & 923 \\ 
2 & 131 & 1077 & 16 & 903 & 25 & 1125 \\ 
1 & 135 & 983 & 18 & 808 & 29 & 1017 \\ 
2 & 139 & 993 & 17 & 852 & 25 & 1004 \\ 
1 & 134 & 1035 & 16 & 870 & 25 & 1137 \\ 
1 & 135 & 1099 & 18 & 908 & 26 & 1137 \\ 
1 & 137 & 977 & 16 & 768 & 25 & 986 \\
\end{tabular}
\newpage
We could use the solution found by the Karmarkar-Karp algorithm as a starting point for the randomized algorithms instead of a random solution. This would have no effect when using pre-partitioning because it obtains a much better solution than KK already. However it would improve the results when using the standard solution representation with the hill climbing and simulated annealing heuristics, because they would at least be bounded by the (superior) KK solution. \\
\\
If we had an implementation of the KK algorithm that recovered the partition, we would get the additional benefit that hill climbing and simulated annealing would have a better starting point from which to find neighbors.
\end{document}  
